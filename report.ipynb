{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a successful dot-com start-ups, after fast growth initially, QWE realized the need for deeper analytical insight into some key business processes, one of which was customer retention. At first, QWE tried to convince the customer to extend the contract by offering free services or discounts on existing services. However, QWE wondered if they could develop a more proactive approach. Also, they hoped they could estimate the probability that a given customer would leave in the near future and identify the drivers that contributed most to that customer’s decision. To solve this problem, QWE wanted to generate a list of the 100 customers who were most likely to leave and, if possible, the three factors contributing most to that likelihood.\n",
    "\n",
    "To collect dataset, QWE rolled back two months to December 1, 2011, and obtained a sample of 6,000 of QWE’s customers as of that date. To start with this task, Customer age, CHI [Customer Happiness Index], and service and usage patterns are thought as the most important characteristics to solve this problem. QWE doubted that those customers with high CHI scores leave much, but those who are unhappy might leave, and so might those for whom CHI scores dropped recently. Also, number of support cases, average support priority, and usage information: logins, blogs, views, and days since last login are related with the customer retention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Initial Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset provided for this analysis includes 6,347 observations, each of which represents information for a given customer, across 13 variables:\n",
    "- ID - The id of a customer\n",
    "- CustomerAgeinmonths  \n",
    "- Churn1Yes0No \n",
    "- CHIScoreMonth0 \n",
    "- CHIScore01 \n",
    "- SupportCasesMonth0 \n",
    "- SupportCases01 \n",
    "- SPMonth0 \n",
    "- SP01 \n",
    "- Logins01 \n",
    "- BlogArticles01 \n",
    "- Views01 \n",
    "- DaysSinceLastLogin01 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Method And Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Is Wall’s belief about the dependence of churn rates on customer age supported by the data? To get some intuition, try visualizing this dependence (Hint: no need to run any statistical tests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(\"data\", \"caseStudyCleaned.csv\")\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    print(\"Download data first!\")\n",
    "\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Churn')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df['CustomerAgeinmonths'],df['Churn1Yes0No'])\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel('Churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph, we can not see any relationship between churn rate and customer age. As a result, customer age can not support the dependence of churn age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 To start, run a single regression model that best predicts the probability that a customer leaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a single regression model, we select the CHIScoreMonth0 to predict the customer churn because it has much higher correlation value than others. \n",
    "\n",
    "|     Characteristic  | Churn1Yes0No |\n",
    "|---------------------|--------------|\n",
    "|ID                   |-0.106701     |\n",
    "|CustomerAgeinmonths  |0.030215      |\n",
    "|Churn1Yes0No         |1.000000      |\n",
    "|CHIScoreMonth0       |-0.084005     |\n",
    "|CHIScore01           |-0.008713     |\n",
    "|SupportCasesMonth0   |-0.044973     |\n",
    "|SupportCases01       |-0.044407     |\n",
    "|SPMonth0             |-0.054935     |\n",
    "|SP01                 |-0.019682     |\n",
    "|Logins01             |-0.043077     |\n",
    "|BlogArticles01       |-0.025090     |\n",
    "|Views01              |0.000007      |\n",
    "|DaysSinceLastLogin01 |0.111568      |\n",
    "\n",
    "Then, we can generate the single regression model: \n",
    "\n",
    "$$ Churn1Yes0No = -2.46064255 - 0.00615342 * CHIScoreMonth0 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. What is the predicted probability that Customer 672 will leave between December 2011 and February 2012? Is that high or low? Did that customer actually leave?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.46064255] [[-0.00615342]]\n",
      "0.033202967751108405\n",
      "0.03502742009007802\n",
      "0.06366613937623102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "myLogMod = LogisticRegression(solver = 'lbfgs').fit(np.array([df.CHIScoreMonth0]).T, np.array([df.Churn1Yes0No]).T)\n",
    "b_1 = myLogMod.coef_\n",
    "b_0 = myLogMod.intercept_\n",
    "print(b_0, b_1)\n",
    "prediction = LogisticRegression(solver = 'lbfgs').fit(np.array([df.CHIScoreMonth0]).T, np.array([df.Churn1Yes0No]).T)\n",
    "list = prediction.predict_proba(np.array([df.CHIScoreMonth0]).T)[:, 1]\n",
    "print(list[671])\n",
    "print(list[353])\n",
    "print(list[5202])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we get the single refression model, we can calculate the the predicted probability with following equation:\n",
    "\n",
    "$$ P(Churn1Yes0No) = \\frac{1}{1 + e^{-(-2.46064255 - 0.00615342 * CHIScoreMonth0)}} $$\n",
    "\n",
    "|   ID   | CHI Score | Probability | Leave |\n",
    "|--------|-----------|-------------|-------|\n",
    "|672     |148        | 3.3%        | NO    |\n",
    "\n",
    "From above list, we can see that the customer 672 actually did not leave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. What about Customers 354 and 5,203?\n",
    "\n",
    "|   ID   | CHI Score | Probability | Leave |\n",
    "|--------|-----------|-------------|-------|\n",
    "|354     |139        | 3.5%        | NO    |\n",
    "|5203    |37         | 6.36%       | NO    |\n",
    "\n",
    "From above list, we can see that the customer 354 and 5203 actually did not leave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 How sensible is the approach with a single model? Can you suggest a better approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.04829851187042081\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test , y_train, y_test = \\\n",
    "    train_test_split(np.array(df['CHIScoreMonth0']), np.array(df['Churn1Yes0No']), test_size = 0.5, random_state = 11)\n",
    "myLinReg = LinearRegression()\n",
    "x = myLinReg.fit(X_train.reshape(-1,1), y_train.reshape(-1,1))\n",
    "linRegPreds = x.predict(X_test.reshape(-1,1))\n",
    "print(\"MSE: \" + str(mean_squared_error(y_test,linRegPreds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector of alphas from 5 billion to 0.005\n",
    "alphas = 10**np.linspace(10, -4, 500)*0.5\n",
    "df_x = df[['CustomerAgeinmonths','CHIScoreMonth0','CHIScore01','SupportCasesMonth0','SupportCases01','SPMonth0','SP01',\n",
    "    'Logins01','BlogArticles01','Views01','DaysSinceLastLogin01']]\n",
    "df_y = df['Churn1Yes0No']\n",
    "X_train, X_test , y_train, y_test = \\\n",
    "    train_test_split(np.array(df_x), np.array(df_y), test_size = 0.5, random_state = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.04799085045280179\n"
     ]
    }
   ],
   "source": [
    "myLinReg = LinearRegression()\n",
    "x = myLinReg.fit(X_train, y_train)\n",
    "linRegPreds = x.predict(X_test)\n",
    "print(\"MSE: \" + str(mean_squared_error(y_test,linRegPreds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(max_iter = 10000, normalize = True)\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(scale(X_train), y_train)\n",
    "\n",
    "    for i in range(X_train.shape[1]):\n",
    "        coefs.append([a, \"b_\" + str(i + 1), lasso.coef_[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>beta</th>\n",
       "      <th>beta_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5450</th>\n",
       "      <td>0.000065</td>\n",
       "      <td>b_6</td>\n",
       "      <td>-0.005218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5451</th>\n",
       "      <td>0.000065</td>\n",
       "      <td>b_7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5452</th>\n",
       "      <td>0.000065</td>\n",
       "      <td>b_8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5453</th>\n",
       "      <td>0.000065</td>\n",
       "      <td>b_9</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5454</th>\n",
       "      <td>0.000065</td>\n",
       "      <td>b_10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5455</th>\n",
       "      <td>0.000065</td>\n",
       "      <td>b_11</td>\n",
       "      <td>0.011148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5456</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>b_1</td>\n",
       "      <td>0.006085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5457</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>b_2</td>\n",
       "      <td>-0.009899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5458</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>b_3</td>\n",
       "      <td>0.002121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5459</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>b_4</td>\n",
       "      <td>-0.001267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5460</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>b_5</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5461</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>b_6</td>\n",
       "      <td>-0.005293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5462</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>b_7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5463</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>b_8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5464</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>b_9</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5465</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>b_10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5466</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>b_11</td>\n",
       "      <td>0.011241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5467</th>\n",
       "      <td>0.000057</td>\n",
       "      <td>b_1</td>\n",
       "      <td>0.006374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5468</th>\n",
       "      <td>0.000057</td>\n",
       "      <td>b_2</td>\n",
       "      <td>-0.010174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5469</th>\n",
       "      <td>0.000057</td>\n",
       "      <td>b_3</td>\n",
       "      <td>0.002457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5470</th>\n",
       "      <td>0.000057</td>\n",
       "      <td>b_4</td>\n",
       "      <td>-0.001405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5471</th>\n",
       "      <td>0.000057</td>\n",
       "      <td>b_5</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5472</th>\n",
       "      <td>0.000057</td>\n",
       "      <td>b_6</td>\n",
       "      <td>-0.005363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5473</th>\n",
       "      <td>0.000057</td>\n",
       "      <td>b_7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5474</th>\n",
       "      <td>0.000057</td>\n",
       "      <td>b_8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5475</th>\n",
       "      <td>0.000057</td>\n",
       "      <td>b_9</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5476</th>\n",
       "      <td>0.000057</td>\n",
       "      <td>b_10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5477</th>\n",
       "      <td>0.000057</td>\n",
       "      <td>b_11</td>\n",
       "      <td>0.011327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5478</th>\n",
       "      <td>0.000053</td>\n",
       "      <td>b_1</td>\n",
       "      <td>0.006672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5479</th>\n",
       "      <td>0.000053</td>\n",
       "      <td>b_2</td>\n",
       "      <td>-0.010525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5480</th>\n",
       "      <td>0.000053</td>\n",
       "      <td>b_3</td>\n",
       "      <td>0.002692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5481</th>\n",
       "      <td>0.000053</td>\n",
       "      <td>b_4</td>\n",
       "      <td>-0.001578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5482</th>\n",
       "      <td>0.000053</td>\n",
       "      <td>b_5</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5483</th>\n",
       "      <td>0.000053</td>\n",
       "      <td>b_6</td>\n",
       "      <td>-0.005456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5484</th>\n",
       "      <td>0.000053</td>\n",
       "      <td>b_7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5485</th>\n",
       "      <td>0.000053</td>\n",
       "      <td>b_8</td>\n",
       "      <td>0.000312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5486</th>\n",
       "      <td>0.000053</td>\n",
       "      <td>b_9</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5487</th>\n",
       "      <td>0.000053</td>\n",
       "      <td>b_10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5488</th>\n",
       "      <td>0.000053</td>\n",
       "      <td>b_11</td>\n",
       "      <td>0.011422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5489</th>\n",
       "      <td>0.000050</td>\n",
       "      <td>b_1</td>\n",
       "      <td>0.006967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5490</th>\n",
       "      <td>0.000050</td>\n",
       "      <td>b_2</td>\n",
       "      <td>-0.010904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5491</th>\n",
       "      <td>0.000050</td>\n",
       "      <td>b_3</td>\n",
       "      <td>0.002865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5492</th>\n",
       "      <td>0.000050</td>\n",
       "      <td>b_4</td>\n",
       "      <td>-0.001715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5493</th>\n",
       "      <td>0.000050</td>\n",
       "      <td>b_5</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5494</th>\n",
       "      <td>0.000050</td>\n",
       "      <td>b_6</td>\n",
       "      <td>-0.005621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5495</th>\n",
       "      <td>0.000050</td>\n",
       "      <td>b_7</td>\n",
       "      <td>0.000198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5496</th>\n",
       "      <td>0.000050</td>\n",
       "      <td>b_8</td>\n",
       "      <td>0.000680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5497</th>\n",
       "      <td>0.000050</td>\n",
       "      <td>b_9</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5498</th>\n",
       "      <td>0.000050</td>\n",
       "      <td>b_10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5499</th>\n",
       "      <td>0.000050</td>\n",
       "      <td>b_11</td>\n",
       "      <td>0.011517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         alpha  beta  beta_val\n",
       "5450  0.000065   b_6 -0.005218\n",
       "5451  0.000065   b_7  0.000000\n",
       "5452  0.000065   b_8  0.000000\n",
       "5453  0.000065   b_9 -0.000000\n",
       "5454  0.000065  b_10  0.000000\n",
       "5455  0.000065  b_11  0.011148\n",
       "5456  0.000061   b_1  0.006085\n",
       "5457  0.000061   b_2 -0.009899\n",
       "5458  0.000061   b_3  0.002121\n",
       "5459  0.000061   b_4 -0.001267\n",
       "5460  0.000061   b_5 -0.000000\n",
       "5461  0.000061   b_6 -0.005293\n",
       "5462  0.000061   b_7  0.000000\n",
       "5463  0.000061   b_8  0.000000\n",
       "5464  0.000061   b_9 -0.000000\n",
       "5465  0.000061  b_10  0.000000\n",
       "5466  0.000061  b_11  0.011241\n",
       "5467  0.000057   b_1  0.006374\n",
       "5468  0.000057   b_2 -0.010174\n",
       "5469  0.000057   b_3  0.002457\n",
       "5470  0.000057   b_4 -0.001405\n",
       "5471  0.000057   b_5 -0.000000\n",
       "5472  0.000057   b_6 -0.005363\n",
       "5473  0.000057   b_7  0.000000\n",
       "5474  0.000057   b_8  0.000000\n",
       "5475  0.000057   b_9 -0.000000\n",
       "5476  0.000057  b_10  0.000000\n",
       "5477  0.000057  b_11  0.011327\n",
       "5478  0.000053   b_1  0.006672\n",
       "5479  0.000053   b_2 -0.010525\n",
       "5480  0.000053   b_3  0.002692\n",
       "5481  0.000053   b_4 -0.001578\n",
       "5482  0.000053   b_5 -0.000000\n",
       "5483  0.000053   b_6 -0.005456\n",
       "5484  0.000053   b_7  0.000000\n",
       "5485  0.000053   b_8  0.000312\n",
       "5486  0.000053   b_9 -0.000000\n",
       "5487  0.000053  b_10  0.000000\n",
       "5488  0.000053  b_11  0.011422\n",
       "5489  0.000050   b_1  0.006967\n",
       "5490  0.000050   b_2 -0.010904\n",
       "5491  0.000050   b_3  0.002865\n",
       "5492  0.000050   b_4 -0.001715\n",
       "5493  0.000050   b_5 -0.000000\n",
       "5494  0.000050   b_6 -0.005621\n",
       "5495  0.000050   b_7  0.000198\n",
       "5496  0.000050   b_8  0.000680\n",
       "5497  0.000050   b_9 -0.000000\n",
       "5498  0.000050  b_10  0.000000\n",
       "5499  0.000050  b_11  0.011517"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myCoefs = pd.DataFrame(coefs, columns=['alpha', 'beta', 'beta_val'])\n",
    "myCoefs.iloc[5450:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Alpha: 5.722114280497082e-05\n",
      "MSE: 0.04792041154813162\n"
     ]
    }
   ],
   "source": [
    "lassocv = LassoCV(cv = 10, normalize = True)\n",
    "lassocv.fit(X_train, y_train)\n",
    "\n",
    "print('Optimal Alpha: ' + str(lassocv.alpha_))\n",
    "\n",
    "lasso.set_params(alpha = lassocv.alpha_)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "print('MSE: ' + str(mean_squared_error(y_test, lasso.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this single model, the MSE is 0.04829851187042081.\n",
    "\n",
    "If we want a better approach to predict the probability that a customer leaves, we can choose Multiple Logistic Regression(MLR). In this way, we can use multiple customer characteristics to predict the probability that a customer leaves.\n",
    "\n",
    "For this approach, we choose three customer characteristics in our \n",
    "\n",
    "- CustomerAgeinmonths \n",
    "- CHIScoreMonth0\n",
    "- CHIScore01\n",
    "- SupportCasesMonth0\n",
    "- SPMonth0\n",
    "- DaysSinceLastLogin01\n",
    "\n",
    "We choose above six features by using ASSSO Regression, after calculation, we can know that Optimal Alpha as 5.722114280497082e-05, and the the MSE is 0.04792041154813162.\n",
    "\n",
    "|alpha    |beta |beta_val  |\n",
    "|---------|-----|----------|\n",
    "|0.000057 |b_1  |0.006374  |\n",
    "|0.000057 |b_2\t|-0.010174 |\n",
    "|0.000057 |b_3\t|0.002457  |\n",
    "|0.000057 |b_4\t|-0.001405 |\n",
    "|0.000057 |b_5\t|-0.000000 |\n",
    "|0.000057 |b_6\t|-0.005363 |\n",
    "|0.000057 |b_7\t|0.000000  |\n",
    "|0.000057 |b_8\t|0.000000  |\n",
    "|0.000057 |b_9\t|-0.000000 |\n",
    "|0.000057 |b_10 |0.000000  |\n",
    "|0.000057 |b_11 |0.011327  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Provide updated estimates of probabilities that Customers 672, 354, and 5,203 will leave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0155824  -0.00609524  0.00339462 -0.04366652 -0.05840514  0.0109126 ]]\n",
      "[-2.80257555]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "prediction = LogisticRegression(solver = 'lbfgs')\\\n",
    "    .fit(np.array([df.CustomerAgeinmonths, df.CHIScoreMonth0,df.CHIScore01, df.SupportCasesMonth0, df.SPMonth0, df.DaysSinceLastLogin01]).T,\n",
    "         np.array([df.Churn1Yes0No]).T)\n",
    "print(prediction.coef_)\n",
    "print(prediction.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.031366885104610935\n",
      "0.034294089568494196\n",
      "0.05487571946497283\n"
     ]
    }
   ],
   "source": [
    "list = prediction.predict_proba(np.array([df.CustomerAgeinmonths, df.CHIScoreMonth0,df.CHIScore01,\n",
    "                                          df.SupportCasesMonth0, df.SPMonth0, df.DaysSinceLastLogin01]).T)[:, 1]\n",
    "print(list[671])\n",
    "print(list[353])\n",
    "print(list[5202])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we select CustomerAgeinmonths, CHIScoreMonth0, CHIScore01, SupportCasesMonth0, SPMonth0, DaysSinceLastLogin01 as varibles in the model, we can get the equation in the Multiple  Logistic Regression(MLR):\n",
    "\n",
    "$$ Churn1Yes0No = -2.80257555 + 0.0155824 * CustomerAgeinmonths - 0.00609524 * CHIScoreMonth0 + 0.00339462 * CHIScore01 - 0.04366652 * SupportCasesMonth0 - 0.05840514 * SPMonth0 + 0.0109126 * DaysSinceLastLogin01 $$\n",
    "\n",
    "Also, the following formula is the equation of the predicted probability:\n",
    "\n",
    "$$ P(Churn1Yes0No) = \\frac{1}{1 + e^{-(-2.80257555 + 0.0155824 * CustomerAgeinmonths - 0.00609524 * CHIScoreMonth0 + 0.00339462 * CHIScore01 - 0.04366652 * SupportCasesMonth0 - 0.05840514 * SPMonth0 + 0.0109126 * DaysSinceLastLogin01)}} $$\n",
    "\n",
    "Now, we can updated estimates of probabilities that Customers 672, 354, and 5,203 will leave:\n",
    "\n",
    "|   ID   | Probability | Leave |\n",
    "|--------|-------------|-------|\n",
    "|672     | 3.13%       | NO    |\n",
    "|354     | 3.42%       | NO    |\n",
    "|5203    | 5.48%       | NO    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. What factors contribute the most to the predicted probabilities that these customers will leave?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the correlation among the customer characteristics, we can find that the factors contribute the most to the predicted probabilities that these customers will leave is DaysSinceLastLogin01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Answer Wall’s “ultimate question”: provide the list of 100 customers with highest churn probabilities and the top three drivers of churn for each customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>2700</td>\n",
       "      <td>0.974376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>1496</td>\n",
       "      <td>0.458863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>133</td>\n",
       "      <td>0.426489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>1863</td>\n",
       "      <td>0.404036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>2563</td>\n",
       "      <td>0.373847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889</th>\n",
       "      <td>1890</td>\n",
       "      <td>0.322436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>871</td>\n",
       "      <td>0.296089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>1522</td>\n",
       "      <td>0.282410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>1181</td>\n",
       "      <td>0.274487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>1108</td>\n",
       "      <td>0.257002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>94</td>\n",
       "      <td>0.250851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3087</th>\n",
       "      <td>3088</td>\n",
       "      <td>0.240645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2280</th>\n",
       "      <td>2281</td>\n",
       "      <td>0.234008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>0.232916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>52</td>\n",
       "      <td>0.217576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>192</td>\n",
       "      <td>0.215051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>1030</td>\n",
       "      <td>0.214446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943</th>\n",
       "      <td>2944</td>\n",
       "      <td>0.212919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>110</td>\n",
       "      <td>0.212340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>166</td>\n",
       "      <td>0.205322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3256</th>\n",
       "      <td>3257</td>\n",
       "      <td>0.202652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>60</td>\n",
       "      <td>0.197249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3026</th>\n",
       "      <td>3027</td>\n",
       "      <td>0.195577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.194620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3580</th>\n",
       "      <td>3581</td>\n",
       "      <td>0.190945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>194</td>\n",
       "      <td>0.186900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>536</td>\n",
       "      <td>0.181380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>2011</td>\n",
       "      <td>0.178349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>1803</td>\n",
       "      <td>0.177937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2078</th>\n",
       "      <td>2079</td>\n",
       "      <td>0.177312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>183</td>\n",
       "      <td>0.139723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>0.139198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>101</td>\n",
       "      <td>0.138786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2991</th>\n",
       "      <td>2992</td>\n",
       "      <td>0.138077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3332</th>\n",
       "      <td>3333</td>\n",
       "      <td>0.137925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.137669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>76</td>\n",
       "      <td>0.137394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>1459</td>\n",
       "      <td>0.134726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>1393</td>\n",
       "      <td>0.134159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>1392</td>\n",
       "      <td>0.133833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2239</th>\n",
       "      <td>2240</td>\n",
       "      <td>0.133775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>1438</td>\n",
       "      <td>0.133293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3685</th>\n",
       "      <td>3686</td>\n",
       "      <td>0.133174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>156</td>\n",
       "      <td>0.133060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>1971</td>\n",
       "      <td>0.132950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671</th>\n",
       "      <td>1672</td>\n",
       "      <td>0.132297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>106</td>\n",
       "      <td>0.132037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2243</th>\n",
       "      <td>2244</td>\n",
       "      <td>0.131538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>123</td>\n",
       "      <td>0.130493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.130451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>2301</td>\n",
       "      <td>0.129347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3694</th>\n",
       "      <td>3695</td>\n",
       "      <td>0.129327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>1143</td>\n",
       "      <td>0.129212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>1199</td>\n",
       "      <td>0.128342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3646</th>\n",
       "      <td>3647</td>\n",
       "      <td>0.128181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>1405</td>\n",
       "      <td>0.128147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>89</td>\n",
       "      <td>0.127998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2234</th>\n",
       "      <td>2235</td>\n",
       "      <td>0.127984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254</th>\n",
       "      <td>2255</td>\n",
       "      <td>0.127984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477</th>\n",
       "      <td>1478</td>\n",
       "      <td>0.127984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID      prob\n",
       "2699  2700  0.974376\n",
       "1495  1496  0.458863\n",
       "132    133  0.426489\n",
       "1862  1863  0.404036\n",
       "2562  2563  0.373847\n",
       "1889  1890  0.322436\n",
       "870    871  0.296089\n",
       "1521  1522  0.282410\n",
       "1180  1181  0.274487\n",
       "1107  1108  0.257002\n",
       "93      94  0.250851\n",
       "3087  3088  0.240645\n",
       "2280  2281  0.234008\n",
       "48      49  0.232916\n",
       "51      52  0.217576\n",
       "191    192  0.215051\n",
       "1029  1030  0.214446\n",
       "2943  2944  0.212919\n",
       "109    110  0.212340\n",
       "165    166  0.205322\n",
       "3256  3257  0.202652\n",
       "59      60  0.197249\n",
       "3026  3027  0.195577\n",
       "0        1  0.194620\n",
       "3580  3581  0.190945\n",
       "193    194  0.186900\n",
       "535    536  0.181380\n",
       "2010  2011  0.178349\n",
       "1802  1803  0.177937\n",
       "2078  2079  0.177312\n",
       "...    ...       ...\n",
       "182    183  0.139723\n",
       "41      42  0.139198\n",
       "100    101  0.138786\n",
       "2991  2992  0.138077\n",
       "3332  3333  0.137925\n",
       "4        5  0.137669\n",
       "75      76  0.137394\n",
       "1458  1459  0.134726\n",
       "1392  1393  0.134159\n",
       "1391  1392  0.133833\n",
       "2239  2240  0.133775\n",
       "1437  1438  0.133293\n",
       "3685  3686  0.133174\n",
       "155    156  0.133060\n",
       "1970  1971  0.132950\n",
       "1671  1672  0.132297\n",
       "105    106  0.132037\n",
       "2243  2244  0.131538\n",
       "122    123  0.130493\n",
       "15      16  0.130451\n",
       "2300  2301  0.129347\n",
       "3694  3695  0.129327\n",
       "1142  1143  0.129212\n",
       "1198  1199  0.128342\n",
       "3646  3647  0.128181\n",
       "1404  1405  0.128147\n",
       "88      89  0.127998\n",
       "2234  2235  0.127984\n",
       "2254  2255  0.127984\n",
       "1477  1478  0.127984\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['prob'] = pd.Series(list, index=df.index)\n",
    "df[['ID', 'prob']].sort_values(by=['prob'], ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is the list of 100 customers with highest churn probabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
